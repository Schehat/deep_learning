{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "dnn_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit ('venv': venv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "interpreter": {
      "hash": "e91cbd2cb1b0beed2d7825a50cfe7ef29fefea5e3a2ce6bac208bae671bdf825"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### imports and setup"
      ],
      "metadata": {
        "id": "I9HqbvOOC-2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import tensorflow as tf\r\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "metadata": {
        "id": "K4DnLv2H1J1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data set"
      ],
      "metadata": {
        "id": "dIejXj-I10zr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_path = tf.keras.utils.get_file(\r\n",
        "    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\r\n",
        "test_path = tf.keras.utils.get_file(\r\n",
        "    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\r\n",
        "\r\n",
        "CSV_COLUMN_NAMES = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\", \"Species\"]\r\n",
        "SPECIES = [\"Setosa\", \"Versicolor\", \"Virginica\"]\r\n",
        "\r\n",
        "train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\r\n",
        "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\r\n",
        "print(train.head())\r\n",
        "\r\n",
        "train_y = train.pop(\"Species\")\r\n",
        "test_y = test.pop(\"Species\")\r\n",
        "print(train.head())\r\n",
        "print(train_y.head())\r\n",
        "\r\n",
        "print(train.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "iG5Wx2RP1182"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## input function"
      ],
      "metadata": {
        "id": "yjKW8rpa6KzW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def input_fn(features, labels, training=True, batch_size=256):\r\n",
        "    # Convert the inputs to a Dataset\r\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\r\n",
        "\r\n",
        "    # Shuffle and repeat if in training mode\r\n",
        "    if training:\r\n",
        "        dataset = dataset.shuffle(1000).repeat()\r\n",
        "    \r\n",
        "    return dataset.batch(batch_size)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-YE4mxaI6NHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## feature columns"
      ],
      "metadata": {
        "id": "QrDa8jvV8PBO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Feature columns describe how to use the input\r\n",
        "my_feature_columns = []\r\n",
        "for key in train.keys():\r\n",
        "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\r\n",
        "print(my_feature_columns)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVKS_7q88PuK",
        "outputId": "214943f1-17ee-4484-e6e9-fd18e231411a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## build model with dnn (deep neural network) & train"
      ],
      "metadata": {
        "id": "_x6C87t29XOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "classifier = tf.estimator.DNNClassifier(\r\n",
        "    feature_columns=my_feature_columns,\r\n",
        "    # Two hidden layers of 30 and 10 nodes\r\n",
        "    hidden_units=[30, 10],\r\n",
        "    # 3 possible classifications\r\n",
        "    n_classes=3)\r\n",
        "\r\n",
        "classifier.train(\r\n",
        "    input_fn=lambda: input_fn(train, train_y, training=True),  # include a lambda to avoid creating an inner function previously in input_fn\r\n",
        "    steps=5000)  # steps: max number of steps (doing gradient descent)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gGWGFLpv9hMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "f182jYc8CER-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# note: not specify steps due to evaluation looks the data only 1 time\r\n",
        "eval_result = classifier.evaluate(\r\n",
        "    input_fn=lambda: input_fn(test, test_y, training=False))\r\n",
        "\r\n",
        "print(\"\\nTest set accuracy: {accuracy:0.3f}\\n\".format(**eval_result))"
      ],
      "outputs": [],
      "metadata": {
        "id": "NJWwy6KKDSwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## predictions"
      ],
      "metadata": {
        "id": "Sh-5fNHkHvuM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# user input based\r\n",
        "def input_fn(features, batch_size=256):\r\n",
        "    # Convert the inputs to a Dataset without labels.\r\n",
        "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\r\n",
        "\r\n",
        "features = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]\r\n",
        "predict = {}\r\n",
        "\r\n",
        "print(\"Please type numeric values as prompted.\")\r\n",
        "for feature in features:\r\n",
        "  valid = True\r\n",
        "  while valid: \r\n",
        "    val = input(feature + \": \")\r\n",
        "    if not val.isdigit(): valid = False\r\n",
        "\r\n",
        "  predict[feature] = [float(val)]\r\n",
        "\r\n",
        "predictions = classifier.predict(input_fn=lambda: input_fn(predict))\r\n",
        "for pred_dict in predictions:\r\n",
        "    class_id = pred_dict[\"class_ids\"][0]\r\n",
        "    probability = pred_dict[\"probabilities\"][class_id]\r\n",
        "\r\n",
        "    print(\"Prediction is '{}' ({:.1f}%)\".format(\r\n",
        "        SPECIES[class_id], 100 * probability))"
      ],
      "outputs": [],
      "metadata": {
        "id": "85uQPvv7Hxjq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Generate predictions from the model\r\n",
        "expected = [\"Setosa\", \"Versicolor\", \"Virginica\"]\r\n",
        "predict_x = {\r\n",
        "    \"SepalLength\": [5.1, 5.9, 6.9],\r\n",
        "    \"SepalWidth\": [3.3, 3.0, 3.1],\r\n",
        "    \"PetalLength\": [1.7, 4.2, 5.4],\r\n",
        "    \"PetalWidth\": [0.5, 1.5, 2.1],\r\n",
        "}\r\n",
        "\r\n",
        "def input_fn(features, batch_size=256):\r\n",
        "    \"\"\"An input function for prediction.\"\"\"\r\n",
        "    # Convert the inputs to a Dataset without labels.\r\n",
        "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\r\n",
        "\r\n",
        "predictions = classifier.predict(\r\n",
        "    input_fn=lambda: input_fn(predict_x))\r\n",
        "\r\n",
        "for pred_dict, expec in zip(predictions, expected):\r\n",
        "    class_id = pred_dict[\"class_ids\"][0]\r\n",
        "    probability = pred_dict[\"probabilities\"][class_id]\r\n",
        "\r\n",
        "    print(\"Prediction is '{}' ({:.1f}%), expected '{}'\".format(\r\n",
        "        SPECIES[class_id], 100 * probability, expec))"
      ],
      "outputs": [],
      "metadata": {
        "id": "rD5bEu1JINfc"
      }
    }
  ]
}